
@book{aho1995,
  title = {{Foundations of Computer Science: C Edition}},
  shorttitle = {{Foundations of Computer Science}},
  author = {Aho, Alfred V. and Ullman, Jeffrey D.},
  year = {1995},
  month = feb,
  edition = {New edition edizione},
  publisher = {{W H Freeman \& Co}},
  address = {{New York}},
  abstract = {This text combines the theoretical foundations of computing with essential discrete mathematics. It follows the same organization as its predecessor, Foundations of Computer Science (also published by W.H. Freeman), with all examples and exercises in C.},
  isbn = {978-0-7167-8284-1},
  keywords = {computer science,introduction}
}

@inproceedings{baadel2016,
  title = {Overlapping Clustering: {{A}} Review},
  shorttitle = {Overlapping Clustering},
  booktitle = {2016 {{SAI Computing Conference}} ({{SAI}})},
  author = {Baadel, Said and Thabtah, Fadi and Lu, Joan},
  year = {2016},
  month = jul,
  pages = {233--237},
  publisher = {{IEEE}},
  address = {{London, United Kingdom}},
  doi = {10.1109/SAI.2016.7555988},
  isbn = {978-1-4673-8460-5}
}

@article{bezdek1984,
  title = {{{FCM}}\textemdash the {{Fuzzy C}}-{{Means}} Clustering-Algorithm},
  author = {Bezdek, James and Ehrlich, Robert and Full, William},
  year = {1984},
  volume = {10},
  pages = {191--203},
  abstract = {This paper transmits a FORTRAN-IV coding of the fuzzy c-means (FCM) clustering program. The FCM program is applicable to a wide variety of geostatistical data analysis problems. This program generates fuzzy partitions and prototypes for any set of numerical data. These partitions are useful for corroborating known substructures or suggesting substructure in unexplored data. The clustering criterion used to aggregate subsets is a generalized least-squares objective function. Features of this program include a choice of three norms (Euclidean, Diagonal, or Mahalonobis), an adjustable weighting factor that essentially controls sensitivity to noise, acceptance of variable numbers of clusters, and outputs that include several measures of cluster validity.},
  journal = {Computers \& Geosciences}
}

@techreport{bodoia,
  title = {{{MapReduce Algorithms}} for K-Means {{Clustering}}},
  author = {Bodoia, Max},
  pages = {11}
}

@book{cormen2009,
  title = {Introduction to {{Algorithms}}, 3rd {{Edition}}},
  author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
  year = {2009},
  month = jul,
  edition = {3rd edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Mass}},
  abstract = {The latest edition of the essential text and professional reference, with substantial new material on such topics as vEB trees, multithreaded algorithms, dynamic programming, and edge-based flow.Some books on algorithms are rigorous but incomplete; others cover masses of material but lack rigor. Introduction to Algorithms uniquely combines rigor and comprehensiveness. The book covers a broad range of algorithms in depth, yet makes their design and analysis accessible to all levels of readers. Each chapter is relatively self-contained and can be used as a unit of study. The algorithms are described in English and in a pseudocode designed to be readable by anyone who has done a little programming. The explanations have been kept elementary without sacrificing depth of coverage or mathematical rigor.The first edition became a widely used text in universities worldwide as well as the standard reference for professionals. The second edition featured new chapters on the role of algorithms, probabilistic analysis and randomized algorithms, and linear programming. The third edition has been revised and updated throughout. It includes two completely new chapters, on van Emde Boas trees and multithreaded algorithms, substantial additions to the chapter on recurrence (now called ``Divide-and-Conquer''), and an appendix on matrices. It features improved treatment of dynamic programming and greedy algorithms and a new notion of edge-based flow in the material on flow networks. Many exercises and problems have been added for this edition. The international paperback edition is no longer available; the hardcover is available worldwide.},
  isbn = {978-0-262-03384-8}
}

@article{dean2008,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2008},
  month = jan,
  volume = {51},
  pages = {107--113},
  issn = {0001-0782, 1557-7317},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper.},
  journal = {Communications of the ACM},
  number = {1}
}

@article{fisher1936,
  title = {The {{Use}} of {{Multiple Measurements}} in {{Taxonomic Problems}}},
  author = {Fisher, R. A.},
  year = {1936},
  volume = {7},
  pages = {179--188},
  issn = {2050-1439},
  doi = {10.1111/j.1469-1809.1936.tb02137.x},
  abstract = {The articles published by the Annals of Eugenics (1925\textendash 1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
  journal = {Annals of Eugenics},
  number = {2}
}

@article{heil2019,
  title = {Advantages of Fuzzy K-Means over k-Means Clustering in the Classification of Diffuse Reflectance Soil Spectra: {{A}} Case Study with {{West African}} Soils},
  shorttitle = {Advantages of Fuzzy K-Means over k-Means Clustering in the Classification of Diffuse Reflectance Soil Spectra},
  author = {Heil, Jannis and H{\"a}ring, Volker and Marschner, Bernd and Stumpe, Britta},
  year = {2019},
  month = mar,
  volume = {337},
  pages = {11--21},
  issn = {00167061},
  doi = {10.1016/j.geoderma.2018.09.004},
  journal = {Geoderma}
}

@book{hubbard2015,
  title = {Data {{Structures}} and {{Algorithms}} with {{Python}}},
  author = {Hubbard, Kent D. Lee;Steve},
  year = {2015},
  publisher = {{Springer}}
}

@book{leskovic2020,
  title = {{Mining of Massive Datasets}},
  author = {Leskovic, Jure and Anand, Rajaraman and Ullman, Jeffrey David},
  year = {2020},
  publisher = {{John Wiley \& Sons}},
  address = {{New Delhi}},
  isbn = {978-1-108-47634-8}
}

@article{lin,
  title = {Data-{{Intensive Text Processing}} with {{MapReduce}}},
  author = {Lin, Jimmy and Dyer, Chris},
  pages = {175}
}

@article{ludwig2015,
  title = {{{MapReduce}}-Based Fuzzy c-Means Clustering Algorithm: Implementation and Scalability},
  shorttitle = {{{MapReduce}}-Based Fuzzy c-Means Clustering Algorithm},
  author = {Ludwig, Simone},
  year = {2015},
  volume = {6},
  abstract = {The management and analysis of big data has been identified as one of the most important emerging needs in recent years. This is because of the sheer volume and increasing complexity of data being created or collected. Current clustering algorithms can not handle big data, and therefore, scalable solutions are necessary. Since fuzzy clustering algorithms have shown to outperform hard clustering approaches in terms of accuracy, this paper investigates the parallelization and scalability of a common and effective fuzzy clustering algorithm named fuzzy c-means (FCM) algorithm. The algorithm is parallelized using the MapReduce paradigm outlining how the Map and Reduce primitives are implemented. A validity analysis is conducted in order to show that the implementation works correctly achieving competitive purity results compared to state-of-the art clustering algorithms. Furthermore, a scalability analysis is conducted to demonstrate the performance of the parallel FCM implementation with increasing number of computing nodes used.},
  journal = {International Journal of Machine Learning and Cybernetics}
}

@misc{marin2020,
  title = {Yelp/Mrjob},
  author = {Marin, David},
  year = {2020},
  month = may,
  url = {https://github.com/Yelp/mrjob},
  abstract = {Run MapReduce jobs on Hadoop or Amazon Web Services},
  howpublished = {Yelp.com}
}

@article{stetco2015,
  title = {Fuzzy {{C}}-Means++: {{Fuzzy C}}-Means with Effective Seeding Initialization},
  shorttitle = {Fuzzy {{C}}-Means++},
  author = {Stetco, Adrian and Zeng, Xiao-Jun and Keane, John},
  year = {2015},
  volume = {42},
  pages = {7541--7548},
  issn = {0957-4174},
  abstract = {Fuzzy C-means has been utilized successfully in a wide range of applications, extending the clustering capability of the K-means to datasets that are uncertain, vague and otherwise hard to cluster. This paper introduces the Fuzzy C-means++ algorithm which, by utilizing the seeding mechanism of the K-means++ algorithm, improves the effectiveness and speed of Fuzzy C-means. By careful seeding that disperses the initial cluster centers through the data space, the resulting Fuzzy C-means++ approach samples starting cluster representatives during the initialization phase. The cluster representatives are well spread in the input space, resulting in both faster convergence times and higher quality solutions. Implementations in R of standard Fuzzy C-means and Fuzzy C-means++ are evaluated on various data sets. We investigate the cluster quality and iteration count as we vary the spreading factor on a series of synthetic data sets. We run the algorithm on real world data sets and to account for the non-determinism inherent in these algorithms we record multiple runs while choosing different k parameter values. The results show that the proposed method gives significant improvement in convergence times (the number of iterations) of up to 40 (2.1 on average) times the standard on synthetic datasets and, in general, an associated lower cost function value and Xie\textendash Beni value. A proof sketch of the logarithmically bounded expected cost function value is given.},
  journal = {Expert Systems with Applications},
  keywords = {Cluster analysis,Fuzzy C-means clustering,Initialization},
  number = {21}
}

@book{tan2005,
  title = {{Introduction to Data Mining}},
  author = {Tan, Pang-Ning and Steinbach, Michael and Kumar, Vipin},
  year = {2005},
  publisher = {{Pearson College Div}},
  address = {{Boston}},
  abstract = {Introduction to Data Mining presents fundamental concepts and algorithms for those learning data mining for the first time. Each major topic is organized into two chapters, beginning with basic concepts that provide necessary background for understanding each data mining technique, followed by more advanced concepts and algorithms.},
  isbn = {978-0-321-32136-7}
}

@inproceedings{zhao2009,
  title = {Parallel {{K}}-{{Means Clustering Based}} on {{MapReduce}}},
  booktitle = {Cloud {{Computing}}},
  author = {Zhao, Weizhong and Ma, Huifang and He, Qing},
  year = {2009},
  pages = {674--679},
  abstract = {Data clustering has been received considerable attention in many applications, such as data mining, document retrieval, image segmentation and pattern classification. The enlarging volumes of information emerging by the progress of technology, makes clustering of very large scale of data a challenging task. In order to deal with the problem, many researchers try to design efficient parallel clustering algorithms. In this paper, we propose a parallel k-means clustering algorithm based on MapReduce, which is a simple yet powerful parallel programming technique. The experimental results demonstrate that the proposed algorithm can scale well and efficiently process large datasets on commodity hardware.},
  isbn = {978-3-642-10665-1},
  keywords = {Data mining,Hadoop,K-means,MapReduce,Parallel clustering},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}


